{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eee9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import FreqDist\n",
    "# from nltk.tag import pos_tag\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import nltk\n",
    "# from nltk.corpus import wordnet\n",
    "# from nltk import FreqDist\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "# from nltk.tag import pos_tag\n",
    "# import re, string\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import math\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import numpy as np\n",
    "from scipy.stats import binom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b198583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def Probabilistic_Method2(unique_words): \n",
    "    h = 2.2*len(unique_words)/(math.log(len(unique_words)))\n",
    "    return  math.ceil(h)\n",
    "    \n",
    "def Random_Resolving_Set(unique_words):\n",
    "    h = Probabilistic_Method2(unique_words)\n",
    "    dictionary = dict()\n",
    "    unique_words_poppy = unique_words\n",
    "    for i in range(1,len(unique_words_poppy)):\n",
    "        dictionary[i] = unique_words_poppy.pop()\n",
    "    r = []\n",
    "    q = []\n",
    "    print(len(dictionary))\n",
    "    for i in range(1, h):\n",
    "        Rlen = binom.rvs(len(dictionary), .5)\n",
    "        for x in range(0,Rlen):\n",
    "            k = random.randint(1, len(dictionary))\n",
    "            r.append(dictionary.get(k))\n",
    "        q.append(r)\n",
    "        r = []\n",
    "    return(q)\n",
    "\n",
    "\n",
    "def Random_Resolving_Set_Compliment(unique_words):\n",
    "    h = Probabilistic_Method2(unique_words)\n",
    "    #print(h)\n",
    "   # dictionary = dict.fromkeys( (range(1,len(unique_words))), unique_words)\n",
    "    #print(dictionary)\n",
    "    dictionary = dict()\n",
    "    #print(len(unique_words))\n",
    "    unique_words_poppy = unique_words\n",
    "    for i in range(1,len(unique_words_poppy)):\n",
    "        #dictionary = dict.fromkeys(unique_words.pop(), i)\n",
    "        dictionary[i] = unique_words_poppy.pop()\n",
    "    r = []\n",
    "    q = []\n",
    "    #print(len(dictionary))\n",
    "    for i in range(1, h):\n",
    "        Rlen = binom.rvs(len(dictionary), .5)\n",
    "        for x in range(0,Rlen):\n",
    "            k = random.randint(1, len(dictionary))\n",
    "            r.append(dictionary.get(k))\n",
    "        q.append(r)\n",
    "        q.append(list(set(dictionary.values())-set(r)))\n",
    "        r = []\n",
    "    return(q)\n",
    "\n",
    "\n",
    "def JaccardSim(a,b):\n",
    "    U = a.union(b)\n",
    "    I = a.intersection(b)\n",
    "    similarity = (len(I)/len(U))\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def JacVector(resolving, test):\n",
    "    M = []\n",
    "    for i in range(len(resolving)):\n",
    "             M.append(JaccardSim(set(resolving[i]),set(test)))\n",
    "    return(M)\n",
    "\n",
    "def JVecEMatrix(resolving,tweets):\n",
    "    veclist = []\n",
    "    for tweet in tweets:\n",
    "        veclist.append(JacVector(resolving,tweet))\n",
    "    #q = scipy.spatial.distance.pdist(veclist, metric='euclidean')\n",
    "    return veclist\n",
    "\n",
    "def PCA_Plotting(data, new_dim):\n",
    "    #planning to add color vector. \n",
    "    pca = PCA(n_components=new_dim)\n",
    "    X_r = pca.fit(data).transform(data)\n",
    "    colors = [\"navy\", \"turquoise\", \"darkorange\"]\n",
    "    lw = 2\n",
    "    X_r=np.transpose(X_r)\n",
    "    plt.scatter(X_r[0],X_r[1])\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    print(pca.n_features_)\n",
    "    print(pca.n_samples_)\n",
    "    print(pca.n_components_)\n",
    "\n",
    "def PCA_Ploting_Dict(data, new_dim, num_tones):\n",
    "    #planning to allow for 3d. currently works only for 2 tones, 2dim. \n",
    "    vec_data = []\n",
    "    tone_data = []\n",
    "    for i in range(0,len(data)):\n",
    "        vec_data.append(data.get(i).get('vec'))\n",
    "        tone_data.append(data.get(i).get('tone'))\n",
    "    pca = PCA(n_components=new_dim,)\n",
    "    X_r = pca.fit(vec_data).transform(vec_data)\n",
    "    lw = 2\n",
    "    X_r=np.transpose(X_r)\n",
    "    plt.figure()\n",
    "    df = pd.DataFrame(dict(first=X_r[0],second = X_r[1],tone = tone_data))\n",
    "    print(df)\n",
    "    colors = {'negative': 'red', 'positive':'blue'}\n",
    "    plt.scatter(df['first'], df['second'],s=3, c=df['tone'].map(colors))\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    print(pca.n_features_)\n",
    "    print(pca.n_samples_)\n",
    "    print(pca.n_components_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e6f1d73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def PCA_Ploting_Dict_Tones_axis_tester(data, new_dim, num_tones, axis):\n",
    "    #planning to allow for 3d. currently works only for , 2dim. \n",
    "    #this version takes in which axis you want to plot. requires a new_dim length vector\n",
    "    print(axis[1])\n",
    "    vec_data = []\n",
    "    tone_data = []\n",
    "    for i in range(0,len(data)):\n",
    "        vec_data.append(data.get(i).get('vec'))\n",
    "        tone_data.append(data.get(i).get('tone'))\n",
    "    \n",
    "    \n",
    "    pca = PCA(n_components=axis[new_dim - 1],) #define a PCA object with chosen dimension number\n",
    "    X_r = pca.fit(vec_data).transform(vec_data) #transform the data to fit it to the pca object\n",
    "    \n",
    "    \n",
    "    #colors = [\"navy\", \"turquoise\", \"darkorange\"]\n",
    "    lw = 2\n",
    "    X_r=np.transpose(X_r) #for plotting\n",
    "    #plt.scatter(X_r[0],X_r[1])\n",
    "    print(X_r)\n",
    "    if(new_dim == 2):\n",
    "        plt.figure()\n",
    "        plt.xlabel(str(axis[0]))\n",
    "        plt.ylabel(str(axis[1]))\n",
    "        colors = [\"navy\", \"turquoise\"]\n",
    "        lw = 2\n",
    "        df = pd.DataFrame(dict(first=X_r[axis[0]-1],second = X_r[axis[1]-1],tone = tone_data))\n",
    "        print(df)\n",
    "        colors = {'negative': 'red', 'positive':'blue', 'neutral': 'brown', 'forth_tone': 'yellow'}\n",
    "        plt.scatter(df['first'], df['second'], c=df['tone'].map(colors))\n",
    "        \n",
    "        print(pca.explained_variance_ratio_)\n",
    "        print(pca.n_features_)\n",
    "        print(pca.n_samples_)\n",
    "        print(pca.n_components_)\n",
    "    \n",
    "    elif(new_dim==3):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        ax.set_xlabel(str(axis[0]))\n",
    "        ax.set_ylabel(str(axis[1]))\n",
    "        ax.set_zlabel(str(axis[2]))#labels the axis choice\n",
    "        colors = [\"navy\", \"turquoise\"]\n",
    "        lw = 13\n",
    "        df = pd.DataFrame(dict(first=X_r[axis[0]-1],second = X_r[axis[1]-1], third = X_r[axis[2]-1], tone = tone_data))\n",
    "        print(df)\n",
    "        colors = {'negative': 'red', 'positive':'blue', 'neutral': 'brown', 'forth_tone': 'yellow'}\n",
    "        ax.scatter(df['first'], df['second'], df['third'], c=df['tone'].map(colors))\n",
    "\n",
    "        print(pca.explained_variance_ratio_)\n",
    "        print(pca.n_features_)\n",
    "        print(pca.n_samples_)\n",
    "        print(pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dddd5ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def k_means_pca_optimizer(data, max_components, num_dimensions, num_clusters, num_tones, tones_list, plotting):\n",
    "    vec_data = []\n",
    "    tone_data = []\n",
    "    score_holder = -2\n",
    "    score_dummy = -2\n",
    "    #pca_matrix holder \n",
    "    \n",
    "    if(num_clusters != 0):\n",
    "        \n",
    "        for i in range(0,len(data)):\n",
    "            vec_data.append(data.get(i).get('vec'))\n",
    "            tone_data.append(data.get(i).get('tone'))\n",
    "\n",
    "\n",
    "        pca = PCA(n_components=max_components,)\n",
    "        X_r = pca.fit(vec_data).transform(vec_data)\n",
    "        \n",
    "        \n",
    "        if (num_dimensions==2):\n",
    "            # need to iterate through to find best PCA axis\n",
    "            for iterator1 in range(1,max_components):\n",
    "                for iterator2 in range(1,max_components) :\n",
    "                    if(iterator1 != iterator2):\n",
    "                        pca_matrix_dummy = []\n",
    "                        pca_matrix_dummy = np.column_stack((X_r[:,iterator1],X_r[:,iterator2]))\n",
    "                        kmeans = KMeans(n_clusters=num_clusters)\n",
    "                        X_cluster = kmeans.fit_predict(pca_matrix_dummy) # this one predicts the clustering\n",
    "                        score_dummy = silhouette_score(pca_matrix_dummy,kmeans.labels_)\n",
    "                        if(score_dummy > score_holder):\n",
    "                            score_holder = score_dummy\n",
    "                            pca_matrix_holder = pca_matrix_dummy\n",
    "                            X_cluster_holder = X_cluster\n",
    "                            axis = [str(iterator1) +\",\"+str(iterator2)]\n",
    "                            axis_for_plot = [str(iterator1),str(iterator2)]\n",
    "                            centers = np.array(kmeans.cluster_centers_)\n",
    "            \n",
    "            if (plotting == True):\n",
    "                #need to figure out how to make this work for arbitrary tones. \n",
    "                plt.figure()\n",
    "                lw = 2\n",
    "                pca_matrix_holder=np.transpose(pca_matrix_holder)\n",
    "                df = pd.DataFrame(dict(first = pca_matrix_holder[0],second = pca_matrix_holder[1],tone = tone_data))\n",
    "                #print(df)\n",
    "                colors_list = ['red','blue','brown','purple','orange','teal','maroon','fuchsia','grey','tan']\n",
    "                colors = {}\n",
    "                dummy_color = 0\n",
    "                for tone_iterator in tones_list :\n",
    "                    colors[tone_iterator] = colors_list[dummy_color]\n",
    "                    dummy_color = dummy_color+1\n",
    "                plt.scatter(df['first'], df['second'], c=df['tone'].map(colors))\n",
    "                #centers = np.transpose(centers)\n",
    "                plt.scatter(centers[:,0], centers[:,1], marker = \"x\" , color = \"black\")\n",
    "                plt.xlabel(axis_for_plot[0])\n",
    "                plt.ylabel(axis_for_plot[1])\n",
    "                plt.title(\"PCA on Tweet Data With\")\n",
    "                #plt.legend()\n",
    "                plt.show\n",
    "\n",
    "        elif(num_dimensions == 3):\n",
    "            for iterator1 in range(1,max_components-1):\n",
    "                for iterator2 in range(1,max_components-1):\n",
    "                    for iterator3 in range(1,max_components-1):\n",
    "                        if(iterator1 != iterator2 and iterator1 != iterator3 and iterator2 != iterator3):\n",
    "                            pca_matrix_dummy = []\n",
    "                            kmeans = KMeans(n_clusters=num_clusters)\n",
    "                            pca_matrix_dummy = np.column_stack((X_r[:,iterator1],X_r[:,iterator2],X_r[:,iterator3]))\n",
    "                            X_cluster = kmeans.fit_predict(pca_matrix_dummy) # this one predicts the clustering\n",
    "                            score_dummy = silhouette_score(X_r,kmeans.labels_)\n",
    "\n",
    "                            if(score_dummy > score_holder):\n",
    "                                score_holder = score_dummy\n",
    "                                pca_matrix_holder = pca_matrix_dummy\n",
    "                                X_cluster_holder = X_cluster\n",
    "                                axis = [str(iterator1)+ \",\"+str(iterator2) + \",\" +str(iterator3)]\n",
    "                                centers = np.array(kmeans.cluster_centers_)\n",
    "                                \n",
    "            if (plotting == True):\n",
    "                x = list(set(tone_data))\n",
    "                dic = dict(zip(x, list(range(-1,len(x)-1)))) #this should work with arbitrary tones. \n",
    "                tone_data_numeric =[dic[v] for v in tone_data]\n",
    "                tone_data_numeric = np.asarray(h)\n",
    "                tone_data_numeric = tone_data_numeric.transpose()\n",
    "                matlab_matrix = np.column_stack((pca_matrix_holder,tone_data_numeric))\n",
    "                mdic = {\"Data\": matlab_matrix, \"label\": \"experiment\"}\n",
    "                filename = r\"C:\\Users\\ajpar\\OneDrive\\Desktop\\Research Documents\\ResearchPython\\Matlab\"  +'\\\\' + str(max_components) + str(num_dimensions) + str(num_clusters) + str(num_tones) + \"function.mat\"\n",
    "                scipy.io.savemat(filename,mdic)\n",
    "                \n",
    "                \n",
    "                \n",
    "                print(\"This functionality needs to be improved. Label centers by hand\")\n",
    "                \n",
    "                #change the positive, negative, neutral to numbers. Then send to matlab using code from evan. \n",
    "                #this should simply replace positive negative and neutrals, then open matlab\n",
    "        else:\n",
    "            print(\"This functionality needs to be added\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Currently,call outside loop to find optimal clusters using returning frame\")\n",
    "    returning_frame = pd.DataFrame(dict(score = score_holder, Axis =  axis, Clusters = len(centers)))\n",
    "    return(returning_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6170f457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6edd57c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049\n",
      "332\n"
     ]
    }
   ],
   "source": [
    "#no mix\n",
    "\n",
    "n = 350 # number of words in each dictionary (n for each tone)\n",
    "lam = 50 # average sentence length\n",
    "imported_vader_words = []\n",
    "negative_vader_list = []\n",
    "neutral_vader_list = []\n",
    "positive_vader_list = []\n",
    "\n",
    "\n",
    "#reading in the words. Change the filepath to suit where yours is stored\n",
    "x = open(r\"C:\\Users\\ajpar\\OneDrive\\Desktop\\Research Documents\\Project Coding\\Sample_Word_Datasets\\Full_Vader_Senti.txt\", \"r\")\n",
    "#e = r\"C:\\Users\\ajpar\\Desktop\\Research Documents\\Project Coding\\Full_Tweet_List_Processed.txt\"\n",
    "for line in x:\n",
    "    \n",
    "    l = line.strip('\\n')\n",
    "    l = l.split(\"\\t\")\n",
    "    imported_vader_words.append(l)\n",
    "    \n",
    "x.close()\n",
    "\n",
    "for y in imported_vader_words : #sorting the words by tone\n",
    "    if (-1<= float(y[1]) <= 1):\n",
    "        neutral_vader_list.append(y[0]) \n",
    "    elif (float(y[1]) < -1):\n",
    "        negative_vader_list.append(y[0])\n",
    "    else :\n",
    "        positive_vader_list.append(y[0])\n",
    "\n",
    "\n",
    "shorten_pos = random.sample(positive_vader_list, n) #gets random n words w/out duplicates\n",
    "shorten_neg = random.sample(negative_vader_list, n) #gets random n words w/out duplicates\n",
    "shorten_neutral = random.sample(neutral_vader_list, n) #gets random n words w/out duplicates\n",
    "\n",
    "\n",
    "negative_sent_list = []#generating the negative sentences. repeats for positive and neutral\n",
    "for i in range(0,500):\n",
    "    h = np.random.poisson(lam=lam, size=None) #chooses the length via a poisson\n",
    "    negative_sent_list.append(random.sample(shorten_neg, h)) #takes a random sample \n",
    "\n",
    "positive_sent_list = []\n",
    "for i in range(0,500):\n",
    "    h = np.random.poisson(lam=lam, size=None)\n",
    "    positive_sent_list.append(random.sample(shorten_pos, h))\n",
    "#print(negative_sent_list)\n",
    "\n",
    "#print(positive_sent_list)\n",
    "\n",
    "neutral_sent_list = []\n",
    "\n",
    "for i in range(0,500):\n",
    "    h = np.random.poisson(lam=lam, size=None)\n",
    "    neutral_sent_list.append(random.sample(shorten_neutral, h))\n",
    "\n",
    "final_list = negative_sent_list + positive_sent_list + neutral_sent_list\n",
    "\n",
    "#getting wordlist \n",
    "wordlist_pure = {x for l in final_list for x in l}\n",
    "wordlist_pure_prime = {x for l in final_list for x in l}\n",
    "\n",
    "#generating resolving set. note that the \"prime\" is for the compliment resolving set, and can be deleted if you don't want to deal with that\n",
    "resolving_pure = Random_Resolving_Set(wordlist_pure)\n",
    "resolving_pure_prime = Random_Resolving_Set_Compliment(wordlist_pure_prime)\n",
    "print(len(resolving_pure))\n",
    "\n",
    "#generating the Jaccard Matrices\n",
    "JacMatrix_Pure = JVecEMatrix(resolving_pure,final_list)\n",
    "JacMatrix_Pure_prime = JVecEMatrix(resolving_pure_prime,final_list)\n",
    "\n",
    "labeled_dic_pure = {}\n",
    "labeled_dic_pure_prime = {}\n",
    "\n",
    "#this just stores all the vectors of the Jmatrix in a dictionary with key vec, and tone corresponding to its tone. \n",
    "for i in range(0,len(JacMatrix_Pure)):\n",
    "    if (i<500):\n",
    "        labeled_dic_pure[i] = {'vec': JacMatrix_Pure[i], 'tone': \"negative\"}\n",
    "    elif (500<i<1000):\n",
    "        labeled_dic_pure[i] = {'vec': JacMatrix_Pure[i], 'tone': \"positive\"}\n",
    "    else:\n",
    "        labeled_dic_pure[i] = {'vec': JacMatrix_Pure[i], 'tone': \"neutral\"}\n",
    "\n",
    "\n",
    "for i in range(0,len(JacMatrix_Pure_prime)):\n",
    "    if (i<500):\n",
    "        labeled_dic_pure_prime[i] = {'vec': JacMatrix_Pure_prime[i], 'tone': \"negative\"}\n",
    "    elif (500<i<1000):\n",
    "        labeled_dic_pure_prime[i] = {'vec': JacMatrix_Pure_prime[i], 'tone': \"positive\"}\n",
    "    else:\n",
    "        labeled_dic_pure_prime[i] = {'vec': JacMatrix_Pure_prime[i], 'tone': \"neutral\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e041448d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e783d7c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049\n",
      "332\n"
     ]
    }
   ],
   "source": [
    "#addition of noise\n",
    "\n",
    "n = 350 # number of words in each dictionary (n for each tone)\n",
    "lam = 50 # average sentence length\n",
    "noise_fraction = .1 #the expected portion of the words in each sentnece that are from the neutral tone.\n",
    "imported_vader_words = []\n",
    "negative_vader_list = []\n",
    "neutral_vader_list = []\n",
    "positive_vader_list = []\n",
    "\n",
    "\n",
    "#reading in the words. Change the filepath to suit where yours is stored\n",
    "x = open(r\"C:\\Users\\ajpar\\OneDrive\\Desktop\\Research Documents\\Project Coding\\Sample_Word_Datasets\\Full_Vader_Senti.txt\", \"r\")\n",
    "#e = r\"C:\\Users\\ajpar\\Desktop\\Research Documents\\Project Coding\\Full_Tweet_List_Processed.txt\"\n",
    "for line in x:\n",
    "    \n",
    "    l = line.strip('\\n')\n",
    "    l = l.split(\"\\t\")\n",
    "    imported_vader_words.append(l)\n",
    "    \n",
    "x.close()\n",
    "\n",
    "for y in imported_vader_words : #sorting the words by tone\n",
    "    if (-1<= float(y[1]) <= 1):\n",
    "        neutral_vader_list.append(y[0]) \n",
    "    elif (float(y[1]) < -1):\n",
    "        negative_vader_list.append(y[0])\n",
    "    else :\n",
    "        positive_vader_list.append(y[0])\n",
    "\n",
    "\n",
    "shorten_pos = random.sample(positive_vader_list, n) #gets random n words w/out duplicates\n",
    "shorten_neg = random.sample(negative_vader_list, n) #gets random n words w/out duplicates\n",
    "shorten_neutral = random.sample(neutral_vader_list, n) #gets random n words w/out duplicates\n",
    "\n",
    "\n",
    "negative_sent_list = []\n",
    "for i in range(0,500):\n",
    "    h = np.random.poisson(lam=lam - lam*noise_fraction, size=None)\n",
    "    k = np.random.poisson(lam= lam*noise_fraction, size=None)\n",
    "    negative_sent_list.append(random.sample(shorten_neg, h) +random.sample(shorten_neutral, k))\n",
    "\n",
    "positive_sent_list = []\n",
    "for i in range(0,500):\n",
    "    #changes so poisson is still same mean with addition\n",
    "    h = np.random.poisson(lam=lam - lam*noise_fraction, size=None)\n",
    "    k = np.random.poisson(lam= lam*noise_fraction, size=None)\n",
    "    positive_sent_list.append(random.sample(shorten_pos, h)+random.sample(shorten_neutral, k))\n",
    "\n",
    "\n",
    "final_list = negative_sent_list + positive_sent_list\n",
    "\n",
    "#getting wordlist \n",
    "wordlist_noise = {x for l in final_list for x in l}\n",
    "wordlist_noise_prime = {x for l in final_list for x in l}\n",
    "\n",
    "#generating resolving set. note that the \"prime\" is for the compliment resolving set, and can be deleted if you don't want to deal with that\n",
    "resolving_noise = Random_Resolving_Set(wordlist_noise)\n",
    "resolving_noise_prime = Random_Resolving_Set_Compliment(wordlist_noise_prime)\n",
    "print(len(resolving_noise))\n",
    "\n",
    "#generating the Jaccard Matrices\n",
    "JacMatrix_noise = JVecEMatrix(resolving_noise,final_list)\n",
    "JacMatrix_noise_prime = JVecEMatrix(resolving_noise_prime,final_list)\n",
    "\n",
    "labeled_dic_noise = {}\n",
    "labeled_dic_noise_prime = {}\n",
    "\n",
    "#this just stores all the vectors of the Jmatrix in a dictionary with key vec, and tone corresponding to its tone. \n",
    "for i in range(0,len(JacMatrix_noise)):\n",
    "    if (i<500):\n",
    "        labeled_dic_noise[i] = {'vec': JacMatrix_noise[i], 'tone': \"negative\"}\n",
    "    elif (500<i<1000):\n",
    "        labeled_dic_noise[i] = {'vec': JacMatrix_noise[i], 'tone': \"positive\"}\n",
    "    else:\n",
    "        labeled_dic_noise[i] = {'vec': JacMatrix_noise[i], 'tone': \"neutral\"} #for now this never triggers, but could for tri tone cross contam\n",
    "\n",
    "\n",
    "for i in range(0,len(JacMatrix_noise_prime)):\n",
    "    if (i<500):\n",
    "        labeled_dic_noise_prime[i] = {'vec': JacMatrix_noise_prime[i], 'tone': \"negative\"}\n",
    "    elif (500<i<1000):\n",
    "        labeled_dic_noise_prime[i] = {'vec': JacMatrix_noise_prime[i], 'tone': \"positive\"}\n",
    "    else:\n",
    "        labeled_dic_noise_prime[i] = {'vec': JacMatrix_noise_prime[i], 'tone': \"neutral\"}#for now this never triggers, but could for tri tone cross contam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39a4063a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699\n",
      "235\n"
     ]
    }
   ],
   "source": [
    "#addition of cross contamination \n",
    "\n",
    "n = 350 # number of words in each dictionary (n for each tone)\n",
    "lam = 50 # average sentence length\n",
    "crossC_fraction = .1 #the expected portion of the words in each sentnece that are from the opposite tone.\n",
    "imported_vader_words = []\n",
    "negative_vader_list = []\n",
    "neutral_vader_list = []\n",
    "positive_vader_list = []\n",
    "\n",
    "\n",
    "#reading in the words. Change the filepath to suit where yours is stored\n",
    "x = open(\"Full_Vader_Senti.txt\", \"r\")\n",
    "#e = r\"C:\\Users\\ajpar\\Desktop\\Research Documents\\Project Coding\\Full_Tweet_List_Processed.txt\"\n",
    "for line in x:\n",
    "    \n",
    "    l = line.strip('\\n')\n",
    "    l = l.split(\"\\t\")\n",
    "    imported_vader_words.append(l)\n",
    "    \n",
    "x.close()\n",
    "\n",
    "for y in imported_vader_words : #sorting the words by tone\n",
    "    if (-1<= float(y[1]) <= 1):\n",
    "        neutral_vader_list.append(y[0]) \n",
    "    elif (float(y[1]) < -1):\n",
    "        negative_vader_list.append(y[0])\n",
    "    else :\n",
    "        positive_vader_list.append(y[0])\n",
    "\n",
    "\n",
    "shorten_pos = random.sample(positive_vader_list, n) #gets random n words w/out duplicates\n",
    "shorten_neg = random.sample(negative_vader_list, n) #gets random n words w/out duplicates\n",
    "shorten_neutral = random.sample(neutral_vader_list, n) #gets random n words w/out duplicates\n",
    "\n",
    "\n",
    "negative_sent_list = []\n",
    "for i in range(0,500):\n",
    "    h = np.random.poisson(lam=lam - lam*crossC_fraction, size=None)\n",
    "    k = np.random.poisson(lam= lam*crossC_fraction, size=None)\n",
    "    negative_sent_list.append(random.sample(shorten_neg, h) +random.sample(shorten_pos, k))\n",
    "\n",
    "positive_sent_list = []\n",
    "for i in range(0,500):\n",
    "    #changes so poisson is still same mean with addition\n",
    "    h = np.random.poisson(lam=lam - lam*crossC_fraction, size=None)\n",
    "    k = np.random.poisson(lam= lam*crossC_fraction, size=None)\n",
    "    positive_sent_list.append(random.sample(shorten_pos, h)+random.sample(shorten_neg, k))\n",
    "\n",
    "\n",
    "final_list = negative_sent_list + positive_sent_list\n",
    "\n",
    "#getting wordlist \n",
    "wordlist_crossC = {x for l in final_list for x in l}\n",
    "wordlist_crossC_prime = {x for l in final_list for x in l}\n",
    "\n",
    "#generating resolving set. note that the \"prime\" is for the compliment resolving set, and can be deleted if you don't want to deal with that\n",
    "resolving_crossC = Random_Resolving_Set(wordlist_crossC)\n",
    "resolving_crossC_prime = Random_Resolving_Set_Compliment(wordlist_crossC_prime)\n",
    "print(len(resolving_crossC))\n",
    "\n",
    "#generating the Jaccard Matrices\n",
    "JacMatrix_crossC = JVecEMatrix(resolving_crossC,final_list)\n",
    "JacMatrix_crossC_prime = JVecEMatrix(resolving_crossC_prime,final_list)\n",
    "\n",
    "labeled_dic_crossC = {}\n",
    "labeled_dic_crossC_prime = {}\n",
    "\n",
    "#this just stores all the vectors of the Jmatrix in a dictionary with key vec, and tone corresponding to its tone. \n",
    "for i in range(0,len(JacMatrix_crossC)):\n",
    "    if (i<500):\n",
    "        labeled_dic_crossC[i] = {'vec': JacMatrix_crossC[i], 'tone': \"negative\"}\n",
    "    elif (500<i<1000):\n",
    "        labeled_dic_crossC[i] = {'vec': JacMatrix_crossC[i], 'tone': \"positive\"}\n",
    "    else:\n",
    "        labeled_dic_crossC[i] = {'vec': JacMatrix_crossC[i], 'tone': \"neutral\"} #for now this never triggers, but could for tri tone cross contam\n",
    "\n",
    "\n",
    "for i in range(0,len(JacMatrix_crossC_prime)):\n",
    "    if (i<500):\n",
    "        labeled_dic_crossC_prime[i] = {'vec': JacMatrix_crossC_prime[i], 'tone': \"negative\"}\n",
    "    elif (500<i<1000):\n",
    "        labeled_dic_crossC_prime[i] = {'vec': JacMatrix_crossC_prime[i], 'tone': \"positive\"}\n",
    "    else:\n",
    "        labeled_dic_crossC_prime[i] = {'vec': JacMatrix_crossC_prime[i], 'tone': \"neutral\"}#for now this never triggers, but could for tri tone cross contam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2b64c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dic_crossC\n",
    "# converting dict to pd.DataFrame \n",
    "df2 = pd.DataFrame.from_dict(labeled_dic_crossC, orient='index')\n",
    "# only pulling out tone because thats our label\n",
    "tones = pd.DataFrame(df2.tone)\n",
    "\n",
    "# convert vectory array to list so that each vector has its own column\n",
    "cross_df = pd.DataFrame(df2.vec.tolist())\n",
    "\n",
    "# Our feature\n",
    "cross_df.head()\n",
    "cross_df = pd.DataFrame(np.hstack([cross_df, tones])) # use np.hstack instead of pd.concat becuase of error issues\n",
    "#rename last column to Tone\n",
    "cross_df.rename(columns={cross_df.columns[235]: \"Tone\"}, inplace=True)\n",
    "cross_df.to_csv('cross-contamination.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cef6af3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54da1bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049\n",
      "332\n"
     ]
    }
   ],
   "source": [
    "#addition of cross contamination and noise\n",
    "\n",
    "n = 350 # number of words in each dictionary (n for each tone)\n",
    "lam = 50 # average sentence length\n",
    "crossC_fraction = .1 #the expected portion of the words in each sentnece that are from the opposite tone.\n",
    "noise_fraction  = .1\n",
    "imported_vader_words = []\n",
    "negative_vader_list = []\n",
    "neutral_vader_list = []\n",
    "positive_vader_list = []\n",
    "\n",
    "\n",
    "#reading in the words. Change the filepath to suit where yours is stored\n",
    "x = open(r\"C:\\Users\\ajpar\\OneDrive\\Desktop\\Research Documents\\Project Coding\\Sample_Word_Datasets\\Full_Vader_Senti.txt\", \"r\")\n",
    "#e = r\"C:\\Users\\ajpar\\Desktop\\Research Documents\\Project Coding\\Full_Tweet_List_Processed.txt\"\n",
    "for line in x:\n",
    "    \n",
    "    l = line.strip('\\n')\n",
    "    l = l.split(\"\\t\")\n",
    "    imported_vader_words.append(l)\n",
    "    \n",
    "x.close()\n",
    "\n",
    "for y in imported_vader_words : #sorting the words by tone\n",
    "    if (-1<= float(y[1]) <= 1):\n",
    "        neutral_vader_list.append(y[0]) \n",
    "    elif (float(y[1]) < -1):\n",
    "        negative_vader_list.append(y[0])\n",
    "    else :\n",
    "        positive_vader_list.append(y[0])\n",
    "\n",
    "\n",
    "shorten_pos = random.sample(positive_vader_list, n) #gets random n words w/out duplicates\n",
    "shorten_neg = random.sample(negative_vader_list, n) #gets random n words w/out duplicates\n",
    "shorten_neutral = random.sample(neutral_vader_list, n) #gets random n words w/out duplicates\n",
    "\n",
    "\n",
    "negative_sent_list = []\n",
    "for i in range(0,500):\n",
    "    h = np.random.poisson(lam=lam - lam*crossC_fraction - lam*noise_fraction, size=None)\n",
    "    k = np.random.poisson(lam= lam*crossC_fraction, size=None)\n",
    "    j = np.random.poisson(lam= lam*noise_fraction, size=None)\n",
    "    negative_sent_list.append(random.sample(shorten_neg, h) +random.sample(shorten_pos, k)+random.sample(shorten_neutral,j))\n",
    "\n",
    "positive_sent_list = []\n",
    "for i in range(0,500):\n",
    "    #changes so poisson is still same mean with addition\n",
    "    h = np.random.poisson(lam=lam - lam*crossC_fraction - lam*noise_fraction, size=None)\n",
    "    k = np.random.poisson(lam= lam*crossC_fraction, size=None)\n",
    "    j = np.random.poisson(lam= lam*noise_fraction, size=None)\n",
    "    positive_sent_list.append(random.sample(shorten_pos, h)+random.sample(shorten_neg, k)+random.sample(shorten_neutral,j))\n",
    "\n",
    "\n",
    "final_list = negative_sent_list + positive_sent_list\n",
    "\n",
    "#getting wordlist \n",
    "wordlist_crossC_Noise = {x for l in final_list for x in l}\n",
    "wordlist_crossC_Noise_prime = {x for l in final_list for x in l}\n",
    "\n",
    "#generating resolving set. note that the \"prime\" is for the compliment resolving set, and can be deleted if you don't want to deal with that\n",
    "resolving_crossC_Noise = Random_Resolving_Set(wordlist_crossC_Noise)\n",
    "resolving_crossC_Noise_prime = Random_Resolving_Set_Compliment(wordlist_crossC_Noise_prime)\n",
    "print(len(resolving_crossC_Noise))\n",
    "\n",
    "#generating the Jaccard Matrices\n",
    "JacMatrix_crossC_Noise = JVecEMatrix(resolving_crossC_Noise,final_list)\n",
    "JacMatrix_crossC_Noise_prime = JVecEMatrix(resolving_crossC_Noise_prime,final_list)\n",
    "\n",
    "labeled_dic_crossC_Noise = {}\n",
    "labeled_dic_crossC_Noise_prime = {}\n",
    "\n",
    "#this just stores all the vectors of the Jmatrix in a dictionary with key vec, and tone corresponding to its tone. \n",
    "for i in range(0,len(JacMatrix_crossC_Noise)):\n",
    "    if (i<500):\n",
    "        labeled_dic_crossC_Noise[i] = {'vec': JacMatrix_crossC_Noise[i], 'tone': \"negative\"}\n",
    "    elif (500<i<1000):\n",
    "        labeled_dic_crossC_Noise[i] = {'vec': JacMatrix_crossC_Noise[i], 'tone': \"positive\"}\n",
    "    else:\n",
    "        labeled_dic_crossC_Noise[i] = {'vec': JacMatrix_crossC_Noise[i], 'tone': \"neutral\"} #for now this never triggers, but could for tri tone cross contam\n",
    "\n",
    "\n",
    "for i in range(0,len(JacMatrix_crossC_Noise_prime)):\n",
    "    if (i<500):\n",
    "        labeled_dic_crossC_Noise_prime[i] = {'vec': JacMatrix_crossC_Noise_prime[i], 'tone': \"negative\"}\n",
    "    elif (500<i<1000):\n",
    "        labeled_dic_crossC_Noise_prime[i] = {'vec': JacMatrix_crossC_Noise_prime[i], 'tone': \"positive\"}\n",
    "    else:\n",
    "        labeled_dic_crossC_Noise_prime[i] = {'vec': JacMatrix_crossC_Noise_prime[i], 'tone': \"neutral\"}#for now this never triggers, but could for tri tone cross contam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca502238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
