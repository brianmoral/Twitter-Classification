{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eee9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07171767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b198583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def Probabilistic_Method2(unique_words): \n",
    "    h = 2.2*len(unique_words)/(math.log(len(unique_words)))\n",
    "    return  math.ceil(h)\n",
    "    \n",
    "def Random_Resolving_Set(unique_words):\n",
    "    h = Probabilistic_Method2(unique_words)\n",
    "    dictionary = dict()\n",
    "    unique_words_poppy = unique_words\n",
    "    for i in range(1,len(unique_words_poppy)):\n",
    "        dictionary[i] = unique_words_poppy.pop()\n",
    "    r = []\n",
    "    q = []\n",
    "    print(len(dictionary))\n",
    "    for i in range(1, h):\n",
    "        Rlen = binom.rvs(len(dictionary), .5)\n",
    "        for x in range(0,Rlen):\n",
    "            k = random.randint(1, len(dictionary))\n",
    "            r.append(dictionary.get(k))\n",
    "        q.append(r)\n",
    "        r = []\n",
    "    return(q)\n",
    "\n",
    "\n",
    "def Random_Resolving_Set_Compliment(unique_words):\n",
    "    h = Probabilistic_Method2(unique_words)\n",
    "    #print(h)\n",
    "   # dictionary = dict.fromkeys( (range(1,len(unique_words))), unique_words)\n",
    "    #print(dictionary)\n",
    "    dictionary = dict()\n",
    "    #print(len(unique_words))\n",
    "    unique_words_poppy = unique_words\n",
    "    for i in range(1,len(unique_words_poppy)):\n",
    "        #dictionary = dict.fromkeys(unique_words.pop(), i)\n",
    "        dictionary[i] = unique_words_poppy.pop()\n",
    "    r = []\n",
    "    q = []\n",
    "    #print(len(dictionary))\n",
    "    for i in range(1, h):\n",
    "        Rlen = binom.rvs(len(dictionary), .5)\n",
    "        for x in range(0,Rlen):\n",
    "            k = random.randint(1, len(dictionary))\n",
    "            r.append(dictionary.get(k))\n",
    "        q.append(r)\n",
    "        q.append(list(set(dictionary.values())-set(r)))\n",
    "        r = []\n",
    "    return(q)\n",
    "\n",
    "\n",
    "def JaccardSim(a,b):\n",
    "    U = a.union(b)\n",
    "    I = a.intersection(b)\n",
    "    similarity = (len(I)/len(U))\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def JacVector(resolving, test):\n",
    "    M = []\n",
    "    for i in range(len(resolving)):\n",
    "             M.append(JaccardSim(set(resolving[i]),set(test)))\n",
    "    return(M)\n",
    "\n",
    "def JVecEMatrix(resolving,tweets):\n",
    "    veclist = []\n",
    "    for tweet in tweets:\n",
    "        veclist.append(JacVector(resolving,tweet))\n",
    "    #q = scipy.spatial.distance.pdist(veclist, metric='euclidean')\n",
    "    return veclist\n",
    "\n",
    "def PCA_Plotting(data, new_dim):\n",
    "    #planning to add color vector. \n",
    "    pca = PCA(n_components=new_dim)\n",
    "    X_r = pca.fit(data).transform(data)\n",
    "    colors = [\"navy\", \"turquoise\", \"darkorange\"]\n",
    "    lw = 2\n",
    "    X_r=np.transpose(X_r)\n",
    "    plt.scatter(X_r[0],X_r[1])\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    print(pca.n_features_)\n",
    "    print(pca.n_samples_)\n",
    "    print(pca.n_components_)\n",
    "\n",
    "def PCA_Ploting_Dict(data, new_dim, num_tones):\n",
    "    #planning to allow for 3d. currently works only for 2 tones, 2dim. \n",
    "    vec_data = []\n",
    "    tone_data = []\n",
    "    for i in range(0,len(data)):\n",
    "        vec_data.append(data.get(i).get('vec'))\n",
    "        tone_data.append(data.get(i).get('tone'))\n",
    "    pca = PCA(n_components=new_dim,)\n",
    "    X_r = pca.fit(vec_data).transform(vec_data)\n",
    "    lw = 2\n",
    "    X_r=np.transpose(X_r)\n",
    "    plt.figure()\n",
    "    df = pd.DataFrame(dict(first=X_r[0],second = X_r[1],tone = tone_data))\n",
    "    print(df)\n",
    "    colors = {'negative': 'red', 'positive':'blue'}\n",
    "    plt.scatter(df['first'], df['second'],s=3, c=df['tone'].map(colors))\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    print(pca.n_features_)\n",
    "    print(pca.n_samples_)\n",
    "    print(pca.n_components_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e6f1d73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def PCA_Ploting_Dict_Tones_axis_tester(data, new_dim, num_tones, axis):\n",
    "    #planning to allow for 3d. currently works only for , 2dim. \n",
    "    #this version takes in which axis you want to plot. requires a new_dim length vector\n",
    "    print(axis[1])\n",
    "    vec_data = []\n",
    "    tone_data = []\n",
    "    for i in range(0,len(data)):\n",
    "        vec_data.append(data.get(i).get('vec'))\n",
    "        tone_data.append(data.get(i).get('tone'))\n",
    "    pca = PCA(n_components=axis[new_dim - 1],)\n",
    "    X_r = pca.fit(vec_data).transform(vec_data)\n",
    "    #colors = [\"navy\", \"turquoise\", \"darkorange\"]\n",
    "    lw = 2\n",
    "    X_r=np.transpose(X_r)\n",
    "    #plt.scatter(X_r[0],X_r[1])\n",
    "    print(X_r)\n",
    "    if(new_dim == 2):\n",
    "        plt.figure()\n",
    "        plt.xlabel(str(axis[0]))\n",
    "        plt.ylabel(str(axis[1]))\n",
    "        colors = [\"navy\", \"turquoise\"]\n",
    "        lw = 2\n",
    "        df = pd.DataFrame(dict(first=X_r[axis[0]-1],second = X_r[axis[1]-1],tone = tone_data))\n",
    "        print(df)\n",
    "        colors = {'negative': 'red', 'positive':'blue', 'neutral': 'brown', 'forth_tone': 'yellow'}\n",
    "        plt.scatter(df['first'], df['second'], c=df['tone'].map(colors))\n",
    "        \n",
    "        print(pca.explained_variance_ratio_)\n",
    "        print(pca.n_features_)\n",
    "        print(pca.n_samples_)\n",
    "        print(pca.n_components_)\n",
    "    \n",
    "    elif(new_dim==3):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        ax.set_xlabel(str(axis[0]))\n",
    "        ax.set_ylabel(str(axis[1]))\n",
    "        ax.set_zlabel(str(axis[2]))#labels the axis choice\n",
    "        colors = [\"navy\", \"turquoise\"]\n",
    "        lw = 13\n",
    "        df = pd.DataFrame(dict(first=X_r[axis[0]-1],second = X_r[axis[1]-1], third = X_r[axis[2]-1], tone = tone_data))\n",
    "        print(df)\n",
    "        colors = {'negative': 'red', 'positive':'blue', 'neutral': 'brown', 'forth_tone': 'yellow'}\n",
    "        ax.scatter(df['first'], df['second'], df['third'], c=df['tone'].map(colors))\n",
    "\n",
    "        print(pca.explained_variance_ratio_)\n",
    "        print(pca.n_features_)\n",
    "        print(pca.n_samples_)\n",
    "        print(pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dddd5ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def k_means_pca_optimizer(data, max_components, num_dimensions, num_clusters, num_tones, tones_list, plotting):\n",
    "    vec_data = []\n",
    "    tone_data = []\n",
    "    score_holder = -2\n",
    "    score_dummy = -2\n",
    "    #pca_matrix holder \n",
    "    \n",
    "    if(num_clusters != 0):\n",
    "        \n",
    "        for i in range(0,len(data)):\n",
    "            vec_data.append(data.get(i).get('vec'))\n",
    "            tone_data.append(data.get(i).get('tone'))\n",
    "\n",
    "\n",
    "        pca = PCA(n_components=max_components,)\n",
    "        X_r = pca.fit(vec_data).transform(vec_data)\n",
    "        \n",
    "        \n",
    "        if (num_dimensions==2):\n",
    "            # need to iterate through to find best PCA axis\n",
    "            for iterator1 in range(1,max_components):\n",
    "                for iterator2 in range(1,max_components) :\n",
    "                    if(iterator1 != iterator2):\n",
    "                        pca_matrix_dummy = []\n",
    "                        pca_matrix_dummy = np.column_stack((X_r[:,iterator1],X_r[:,iterator2]))\n",
    "                        kmeans = KMeans(n_clusters=num_clusters)\n",
    "                        X_cluster = kmeans.fit_predict(pca_matrix_dummy) # this one predicts the clustering\n",
    "                        score_dummy = silhouette_score(pca_matrix_dummy,kmeans.labels_)\n",
    "                        if(score_dummy > score_holder):\n",
    "                            score_holder = score_dummy\n",
    "                            pca_matrix_holder = pca_matrix_dummy\n",
    "                            X_cluster_holder = X_cluster\n",
    "                            axis = [str(iterator1) +\",\"+str(iterator2)]\n",
    "                            axis_for_plot = [str(iterator1),str(iterator2)]\n",
    "                            centers = np.array(kmeans.cluster_centers_)\n",
    "            \n",
    "            if (plotting == True):\n",
    "                #need to figure out how to make this work for arbitrary tones. \n",
    "                plt.figure()\n",
    "                lw = 2\n",
    "                pca_matrix_holder=np.transpose(pca_matrix_holder)\n",
    "                df = pd.DataFrame(dict(first = pca_matrix_holder[0],second = pca_matrix_holder[1],tone = tone_data))\n",
    "                #print(df)\n",
    "                colors_list = ['red','blue','brown','purple','orange','teal','maroon','fuchsia','grey','tan']\n",
    "                colors = {}\n",
    "                dummy_color = 0\n",
    "                for tone_iterator in tones_list :\n",
    "                    colors[tone_iterator] = colors_list[dummy_color]\n",
    "                    dummy_color = dummy_color+1\n",
    "                plt.scatter(df['first'], df['second'], c=df['tone'].map(colors))\n",
    "                #centers = np.transpose(centers)\n",
    "                plt.scatter(centers[:,0], centers[:,1], marker = \"x\" , color = \"black\")\n",
    "                plt.xlabel(axis_for_plot[0])\n",
    "                plt.ylabel(axis_for_plot[1])\n",
    "                plt.title(\"PCA on Tweet Data With\")\n",
    "                #plt.legend()\n",
    "                plt.show\n",
    "\n",
    "        elif(num_dimensions == 3):\n",
    "            for iterator1 in range(1,max_components-1):\n",
    "                for iterator2 in range(1,max_components-1):\n",
    "                    for iterator3 in range(1,max_components-1):\n",
    "                        if(iterator1 != iterator2 and iterator1 != iterator3 and iterator2 != iterator3):\n",
    "                            pca_matrix_dummy = []\n",
    "                            kmeans = KMeans(n_clusters=num_clusters)\n",
    "                            pca_matrix_dummy = np.column_stack((X_r[:,iterator1],X_r[:,iterator2],X_r[:,iterator3]))\n",
    "                            X_cluster = kmeans.fit_predict(pca_matrix_dummy) # this one predicts the clustering\n",
    "                            score_dummy = silhouette_score(X_r,kmeans.labels_)\n",
    "\n",
    "                            if(score_dummy > score_holder):\n",
    "                                score_holder = score_dummy\n",
    "                                pca_matrix_holder = pca_matrix_dummy\n",
    "                                X_cluster_holder = X_cluster\n",
    "                                axis = [str(iterator1)+ \",\"+str(iterator2) + \",\" +str(iterator3)]\n",
    "                                centers = np.array(kmeans.cluster_centers_)\n",
    "                                \n",
    "            if (plotting == True):\n",
    "                x = list(set(tone_data))\n",
    "                dic = dict(zip(x, list(range(-1,len(x)-1)))) #this should work with arbitrary tones. \n",
    "                tone_data_numeric =[dic[v] for v in tone_data]\n",
    "                tone_data_numeric = np.asarray(h)\n",
    "                tone_data_numeric = tone_data_numeric.transpose()\n",
    "                matlab_matrix = np.column_stack((pca_matrix_holder,tone_data_numeric))\n",
    "                mdic = {\"Data\": matlab_matrix, \"label\": \"experiment\"}\n",
    "                filename = r\"C:\\Users\\ajpar\\OneDrive\\Desktop\\Research Documents\\ResearchPython\\Matlab\"  +'\\\\' + str(max_components) + str(num_dimensions) + str(num_clusters) + str(num_tones) + \"function.mat\"\n",
    "                scipy.io.savemat(filename,mdic)\n",
    "                \n",
    "                \n",
    "                \n",
    "                print(\"This functionality needs to be improved. Label centers by hand\")\n",
    "                \n",
    "                #change the positive, negative, neutral to numbers. Then send to matlab using code from evan. \n",
    "                #this should simply replace positive negative and neutrals, then open matlab\n",
    "        else:\n",
    "            print(\"This functionality needs to be added\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Currently,call outside loop to find optimal clusters using returning frame\")\n",
    "    returning_frame = pd.DataFrame(dict(score = score_holder, Axis =  axis, Clusters = len(centers)))\n",
    "    return(returning_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6edd57c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#no mix\n",
    "\n",
    "#Devoted to importing and getting wordlist using vader lexicon\n",
    "#https://github.com/cjhutto/vaderSentiment#code-examples\n",
    "\n",
    "# note, I'm not bothering to do stemming and lemmatizing on this stuff, since in terms of a toy example it shouldnt matter\n",
    "#if this is to be used in the future, will have to write code to deal with words w/ same token (hope vs hopeful)\n",
    "#being in different lists. Could result in conflict. \n",
    "n = 350\n",
    "imported_vader_words = []\n",
    "negative_vader_list = []\n",
    "neutral_vader_list = []\n",
    "positive_vader_list = []\n",
    "lam_vary = {}\n",
    "lam_vary_prime = {}\n",
    "x = open(r\"C:\\Users\\ajpar\\OneDrive\\Desktop\\Research Documents\\Project Coding\\Sample_Word_Datasets\\Full_Vader_Senti.txt\", \"r\")\n",
    "#e = r\"C:\\Users\\ajpar\\Desktop\\Research Documents\\Project Coding\\Full_Tweet_List_Processed.txt\"\n",
    "for line in x:\n",
    "    \n",
    "    l = line.strip('\\n')\n",
    "    l = l.split(\"\\t\")\n",
    "    imported_vader_words.append(l)\n",
    "    \n",
    "x.close()\n",
    "\n",
    "for y in imported_vader_words :\n",
    "    if (-1<= float(y[1]) <= 1):\n",
    "        neutral_vader_list.append(y[0]) \n",
    "    elif (float(y[1]) < -1):\n",
    "        negative_vader_list.append(y[0])\n",
    "    else :\n",
    "        positive_vader_list.append(y[0])\n",
    "\n",
    "\n",
    "shorten_pos = random.sample(positive_vader_list, n) #gets random n words w/out duplicates\n",
    "shorten_neg = random.sample(negative_vader_list, n) #gets random n words w/out duplicates\n",
    "shorten_neutral = random.sample(neutral_vader_list, n) #gets random n words w/out duplicates\n",
    "\n",
    "for lam in range(5,100,5):\n",
    "    negative_sent_list = []\n",
    "    for i in range(0,500):\n",
    "        h = np.random.poisson(lam=lam, size=None)\n",
    "        negative_sent_list.append(random.sample(shorten_neg, h))\n",
    "\n",
    "    positive_sent_list = []\n",
    "    for i in range(0,500):\n",
    "        h = np.random.poisson(lam=lam, size=None)\n",
    "        positive_sent_list.append(random.sample(shorten_pos, h))\n",
    "    #print(negative_sent_list)\n",
    "\n",
    "    #print(positive_sent_list)\n",
    "\n",
    "    neutral_sent_list = []\n",
    "\n",
    "    for i in range(0,500):\n",
    "        h = np.random.poisson(lam=lam, size=None)\n",
    "        neutral_sent_list.append(random.sample(shorten_neutral, h))\n",
    "\n",
    "    final_list = negative_sent_list + positive_sent_list + neutral_sent_list\n",
    "\n",
    "\n",
    "    test_words2 = {x for l in final_list for x in l}\n",
    "    test_words2_prime = {x for l in final_list for x in l}\n",
    "    resolve_test2 = Random_Resolving_Set(test_words2)\n",
    "    resolve_test2_prime = Random_Resolving_Set_Compliment(test_words2_prime)\n",
    "    print(len(resolve_test2))\n",
    "\n",
    "    x_test2 = JVecEMatrix(resolve_test2,final_list)\n",
    "    x_test2_prime = JVecEMatrix(resolve_test2_prime,final_list)\n",
    "    #print(x_test)\n",
    "\n",
    "    vec_dic_test7 = {}\n",
    "    vec_dic_test7_prime = {}\n",
    "    for i in range(0,len(x_test2)):\n",
    "        if (i<500):\n",
    "            vec_dic_test7[i] = {'vec': x_test2[i], 'tone': \"negative\"}\n",
    "        elif (500<i<1000):\n",
    "            vec_dic_test7[i] = {'vec': x_test2[i], 'tone': \"positive\"}\n",
    "        else:\n",
    "            vec_dic_test7[i] = {'vec': x_test2[i], 'tone': \"neutral\"}\n",
    "        \n",
    "    lam_vary[lam] = vec_dic_test7\n",
    "    for i in range(0,len(x_test2_prime)):\n",
    "            if (i<500):\n",
    "                vec_dic_test7_prime[i] = {'vec': x_test2_prime[i], 'tone': \"negative\"}\n",
    "            elif (500<i<1000):\n",
    "                vec_dic_test7_prime[i] = {'vec': x_test2_prime[i], 'tone': \"positive\"}\n",
    "            else:\n",
    "                vec_dic_test7_prime[i] = {'vec': x_test2_prime[i], 'tone': \"neutral\"}\n",
    "    lam_vary_prime[lam] = vec_dic_test7_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e783d7c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1229ddd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a4063a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da1bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
